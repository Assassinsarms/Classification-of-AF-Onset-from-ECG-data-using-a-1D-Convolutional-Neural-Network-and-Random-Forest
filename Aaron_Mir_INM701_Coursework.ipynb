{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.1 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "2db524e06e9f5f4ffedc911c917cb75e12dbc923643829bf417064a77eb14d37"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# INM701 Coursework\n",
    "### Aaron Mir (Student Number: 160001207)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "In this coursework, you are expected to demonstrate what you have learned in the module by applying artificial intelligence techniques as covered in the module to a dataset and domain of your choice. This will include some or all of:\n",
    "\n",
    "    • Define the domain and dataset(s) (you are free to choose the domain and the dataset that you want to investigate).\n",
    "\n",
    "    • Define  questions and analysis tasks (a brief overview  of  the  domain, analytical questions that are being asked, a list of your objectives and the \n",
    "    expected output(s)of your analysis)\n",
    "    \n",
    "    • Perform an initial investigation of the dataset and the characteristics of the data. Develop a viable plan: which  data  processing  steps  you  will  need  to\n",
    "    perform, how you will transform the data to make it useable, which artificial intelligence techniques you can potentially use and what sorts of potential\n",
    "    observations these can lead to.\n",
    "\n",
    "    • Perform the analysis.  Get the data ready for analysis, carry out your analysis/modelling as  needed,  validate your  results  and  communicate observations, \n",
    "    iterating through this process. Analytical operations can include data processing to an extent that is needed (not all datasets are messy) to prepare a useful and\n",
    "    robust dataset to work within, and data derivation (such as feature engineering).\n",
    "\n",
    "    • Split your dataset (train/validate/test, somedatasets come pre-split). If you have a holdout test set then you most likely don’t want to use this until the near \n",
    "    the end of your work.\n",
    "\n",
    "    • You might establish a baseline result first, computing metrics on training and validation sets,  analyse  errors, work  on  succeeding iterations, and \n",
    "    alternative models. (If initial metrics are amazing and there are no errors is the problem too easy?)\n",
    "\n",
    "    • Be  close to your data  (visualise  the  dataset,  collect  summary  statistics,  look  at  errors, analyse how different parameters affect performance, try \n",
    "    out different model variants).\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Classification of Atrial Fibrillation from ECG data using a 1D Convolutional Neural Network \n",
    "Goal: Use the Paroxysmal Atrial Fibrillation Prediction Challenge Database to build a model that can classify an Electrocardiogram (ECG) signal as having Atrial Fibrillation (AF) or Normal Sinus Rhythm (NSR). This database contains a learning set(which I have renamed to Normal and AF) and a testing set. The labels of the testing set are not known so in order to test the model, the training set will be split into training and testing sets. \n",
    "\n",
    "The records titled with AF come from patients who have paroxysmal AF and those titled with Normal come from subjects who have NSR.\n",
    "\n",
    "Each record contains a 30-minute ECG record.\n",
    "\n",
    "Extra goal: Use model on other databases\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "import os\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow.keras.utils\n",
    "from tensorflow.keras.models import Sequential\n",
    "#from tensorflow.keras.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Python: 3.8.1 (tags/v3.8.1:1b293b6, Dec 18 2019, 23:11:46) [MSC v.1916 64 bit (AMD64)]\nNumpy Version: 1.18.1\nTesorflow Version: 2.3.0\nKeras Version: 2.4.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"Numpy Version: {np.__version__}\")\n",
    "print(f\"Tesorflow Version: {tensorflow.__version__}\")\n",
    "print(f\"Keras Version: {tensorflow.keras.__version__}\")"
   ]
  },
  {
   "source": [
    "## Data Preprocessing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['NSR', 'AF'] "
   ]
  },
  {
   "source": [
    "## Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}